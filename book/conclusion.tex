\chapter{Conclusion}
\chapterprecis{Abraham D. Flaxman, Christopher J. L. Murray, and Theo Vos}

This book has developed our new metaregression framework for
descriptive epidemiology from first principles, and applied it to
several examples.  This approach is new and necessarily complicated.
However, in exposition we strove to make it no more complicated than
necessary.

There are $7$ ways that this approach differs from traditional
metaregression methods, each addressing commonly occurring features of
the data collected in systematic review during GBD 2010.
Recall that the data are often very \emph{sparse} and very
\emph{noisy}.  When there are whole regions of the globe for which no
data are available; when there are not measurements of prevalence, but
only of incidence; when the regions that do have data have
measurements that vary 10 times more than sampling error would allow;
and in many combinations of these challenging environments, we must
produce estimates that reflect the uncertainty of the available data.

Our approach has used:
\begin{itemize}
\item The negative binomial model of data, an approach that allows groups to have
observations of zero and dispersion beyond that seen in Poisson and binomial models.

\item A piecewise linear spline model of age-specific hazards, which balances computational
tractability and age-patten flexibility.

\item Bayesian methods, to quantify uncertainty and incorporate priors
  based on biology, exposure, or clinical series.

\item The age-standardizing model of heterogeneous, nonstandard age groups, such as $18$--$35$
or $15$ and older.

\item Fixed effects modeling to crosswalk between available studies that use
different case definitions, to predict out-of-sample, based on known
country-specific covariates, and to quantify differences in
nonsampling variance in different study types.

\item Random effects modeling to capture true variation within and between regions.

\item Integrative systems modeling to combine data collected for many different outcomes,
such as incidence, prevalence, remission, excess mortality, or
cause-specific mortality.
\end{itemize}

There are a number of important extensions to this approach and other
avenues for future work that should be pursued in the future.  These
have been highlighted at the end of each chapter in Part I of the
book, and are all collected here, as a plan for how to move forward.

Now that systematic reviews have been conducted for numerous
conditions, the data gathered in the reviews can be used for a
systematic out-of-sample cross-validation exercise to refine modeling
choices and suggest new improvements.

Further research into the rate models from
chapter~\ref{theory-rate_model} can continue to develop and test the
offset log-transformed model and see how it compares to the negative
binomial model on a range of models collected through systematic
review.  Perhaps with additional innovation in computation algorithms,
the beta-binomial model can also be made efficient enough to be
seriously considered as well, but this seems to require methods beyond
the MCMC approach of chapter~\ref{numerical-algorithms}.

The age-specific hazard functions in
chapter~\ref{theory-age_pattern_model} have more model parameters than
ideal.  Future work should be dedicated to removing the necessity of
spline-modeling decisions about the number of knots, location of
knots, and level of smoothing.  If computational algorithms become
fast enough, this could be accomplished simply by exploring a range of
parameters and selecting (or averaging) based on out-of-sample
predictive validity.  An alternative line of research would be to go
from spline models to Gaussian processes, or some related
nonparametric formulation of age-specific hazard function.  This will
certainly have its own computational challenges.

The priors in chapter~\ref{theory-expert_priors} can be extended
through three directions of future work. First, the expert priors
deserve an automated and fool-proof procedure for sensitivity
analysis, so the impact of these assumptions can be assessed quickly
and comprehensively. Second, there are additional formulations of
expert priors which would be helpful in certain settings, such as the
unimodality prior described at the end of that chapter.  Third, and
most importantly, the empirical priors that have been used so far must
be compared to alternative formulations, particularly in terms of the
assumptions about the variance-covariance matrix for the joint
distribution of hazards at different ages.

Age group modeling and covariate modeling can be extended to consider
alternative models of heterogeneous age groups, uncertainty, and
missingness in predictive covariates.  Based on the success of
ensemble modeling for covariates in other settings, that approach
could also be considered here, but only if the computation time for
individual models in the ensemble can be sped up substantially first.

Removing the endemic equlibrium assumption from
section~\ref{theory-forward_sim-compartmental_model-simplying_assumptions}
is another important direction for future work.  The available data
for most global descriptive epidemiology is too sparse and noisy to justify
this complication, but for many national and subnational analyses,
this will be highly relevant.


All of these directions for future work will be facilitated by
research into numerical algorithms and computational infrastructure,
since the time it takes to run these models is currently an impediment
to large cross-validation exercises as well as searching for optimal
parameter settings through out-of-sample predictive validity.

If substantial speed increases are possible through improved algorithms and
infrastructure, it may also be possible to go beyond the empirical Bayes
approach and to fit a full hierarchical model for all regions (or
countries) simultaneously.  This will require innovation in models as
well as methods to determine the most appropriate way to formalize the
hierarchical similarity priors.  It is likely to be extremely
demanding computationally, however, and it could be that experimenting
with alternative approaches to the empirical prior model will yield an
intermediate approach that requires only a more modest increase in
computational power.

%%d3qa for systematic review as well as tools for analysist to check
%%their data, key lesson learned.  example with chernoff faces.

%%value of information analysis

%%routine use of oos-pv in model building and covariate selection

%% An area of future research Chris suggested is the ability to
%% create posterior at the country level.  However, I think you covered
%% this in the paragraph above.

Besides computational algorithms and speed, another area of research
is to create more tools for the analyst to compare different rate
types and the effects of modeling decisions.  In particular,
developing a way to determine the effect of the priors and the effect
of the data on the posterior estimate would be valuable for model
checking.

The development of a metaregression approach for descriptive
epidemiological prediction has been quite an adventure.  It is likely
to continue to be in the future as data, models, methods, and
infrastructure change and improve, making even more precise and
accurate estimation possible.
