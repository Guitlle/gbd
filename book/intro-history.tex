\section{History of Generic Disease Modeling}
Computing power has increased dramatically over the 20 year period that the DisMod family of generic disease modeling software has evolved. The precursor to the first DisMod, the Harvard Incidence-Prevalence Model, was a spreadsheet implemented in Lotus 123 (Murray & Lopez, 1994). DisMod I, the software used for the first Global Burden of Disease Study, was a C++ program that ran in Microsoft Windows 3.1 on a desktop computer (Murray & Lopez, The global burden of disease: a comprehensive assessment of mortality and disability from diseases, injuries, and risk factors in 1990 and projected to 2020, 1996). DisMod II provided more control over inputs to the model and provided a graphical user interface (Barendregt, Oortmarssen, Vos, & Murray, 2003). DisMod III, the latest member of the DisMod family and the subject of this book, consists of an interactive web interface to a computational engine that runs on a roughly 2000 core computer cluster. From a spreadsheet solving a simple system of differential equations to a computer cluster performing Markov Chain Monte Carlo to compute the probability distributions of epidemiological parameters in a dynamic system, generic disease modeling has come a long way. 
Much of the initial work on generic disease modeling was pioneered by Christopher Murray and Alan Lopez as part of the original Global Burden of Disease Study (GBD 1990) commissioned by the World Bank in 1991 (IHME, 2011). One of their first models was the Harvard Incidence-Prevalence Model. The model involved constructing a life table to simulate a cohort exposed to a set of age-specific incidence, remission, case fatality and background mortality hazards. At each year in the life table, the model simulated a simple 3-compartment model to provide estimates of the number susceptible, the number of cases and the number of deaths to input into the life table for the next year (Murray & Lopez, Quantifying disability: data, methods, and results, 1994). Murray and Lopez delineated 3 purposes for this model: 1) To estimate incidence and duration by age if prevalence is known, 2) To estimate prevalence when incidence is known and 3) To estimate death attributable to different causes of diseases. The estimation procedure in all 3 cases involved setting the unknown parameters to a level deemed reasonable by the researchers, running the model with these new parameters, evaluating the plausibility of the results, and then repeating the entire process with a modified set of parameters until a plausible and consistent set of estimates are obtained. 
The first DisMod, used for GBD 1990, adhered closely to the modeling strategy of the Harvard Incidence-Prevalence Model (Murray & Lopez, The global burden of disease: a comprehensive assessment of mortality and disability from diseases, injuries, and risk factors in 1990 and projected to 2020, 1996). It was developed by Murray and Lopez along with colleagues in the Burden of Disease Unit at the Harvard Center for Population and Development Studies. It used a 4-compartment generic disease model , where members of the population could move from the “Susceptibles” compartment to the “Cases” compartment if they contracted the disease (and back if they remitted) to the “Cause-specific deaths” compartment if they died of the disease or to the “Deaths from general mortality” compartment if they died of general, background mortality. DisMod I allowed a user to specify a set of age-specific incidence, remission and case-fatality rates for a specific disease, region and sex. It then would solve the differential equations corresponding to the 4-compartment model with the user-specified hazards. 
For the second iteration of the Global Burden of Disease Study, Jan Bardendregt and colleagues developed a new implementation of DisMod, which provided a stable user-interface and sufficient documentation for the application to be used widely beyond the initial analysis (Barendregt, Oortmarssen, Vos, & Murray, 2003). DisMod II, like the original DisMod focused on data confrontation, and specialized in combining single best-estimates of individual disease parameters, such as incidence, prevalence, remission, and case-fatality to produce a set of internally consistent estimates for a single time, place, and sex. 
As is often the case in science, a very similar approach had been developed previously, by researchers, totally unknown to the WHO researchers, at the International Institute for Applied Systems Analysis in Austria in the 1970s. These researchers worked on developing a generic Healthcare System Model to improve management and planning in the health sector. One component of this model was a computer program to estimate prevalence from incidence (Klementiev, 1977). That program evolved a population exposed to age-specific incidences of disease and death through time. It was applied to estimate the prevalence of malignant neoplasm in Austria, France and Belgium. 
The generic disease modeling approach was distributed publicly by WHO and used widely in burden of disease studies. These studies adopted the methodology of the global study, but aimed to assess burden at a level of detail more relevant for national policymakers. Over 2 dozen countries have undertaken a national burden of disease study (WHO, 2003). The first generation of these studies, which used the same valuations for the disability weight of living in different health states as the original Global Burden of Disease study, included studies in Mexico, Chile, Columbia and Mauritius (Lozano, Murray, Frenk, & Bobadilla, 1995) (Concha, Aguilera, & Albala, 1996) (Salud, 1994) (Vos, Timaeus, Gareeboo, Roussety, Huttly, & Murray, 1995). The Mauritius Burden of Disease Study was led by Theo Vos, who became a key contributer to later iterations of DisMod. The Australian Burden of Disease Study is a more recent example of national studies using DisMod to make data on epidemiological parameters consistent (Mathers, Vos, Stevenson, & Begg, 2001). Despite its wide application, there were some feature requests and methodological concerns that developed over the decade that DisMod II reigned as the standard approach for disease burden estimation.
Chief among them was the difficultly in producing consistent estimates that exhibited ``face validity'', for example age patterns that increased monotonically as a function of age. Despite strongly held prior beliefs on the part of domain experts, it was not uncommon for the data to show oscillations as a function of age, due to the contortions that DisMod II would subject rates to in order to produce consistent estimates as close to the inconsistent input estimates as possible.
Another important challenge in the DisMod II workflow was the production of single best estimates for at least 3 independent rates in the first place. As mentioned in section TK, for common diseases like TK, there are over TK studies of disease prevalence that met the inclusion criteria of a recent systematic literature review. DisMod II provided no guidance on how to go from this large collection of estimates, often for incommensurate age intervals, to a single best estimate of disease prevalence.
Finally, although DisMod II excelled in providing consistent estimates from the confrontation of inconsistent estimates of several disease parameters for a single place and time, it was laborious on the part of the data analyst to produce comparable estimates for a variety of different places and times. In the Global Burden of Disease 2010 Study, there are 21 geographic regions to produce estimates for, at 3 different points in time, for males and females. Even an analysis that is trivial for one region/time/sex becomes burdensome when it must be replicated 126 times.
In 2008, work on DisMod III was initiated by Abraham Flaxman. The project was inspired by the previous generations of generic disease models developed in the past global and national burden of disease studies, as well as work on mortality estimation and prediction by Girosi and King, which used Bayesian methods to estimate age patterns of mortality simultaneously for multiple regions of the world. The model was developed in collaboration with the many disease experts who volunteered to help in the GBD 2010 study. The expert contributors in the mental disorders group and substance dependence group produced preliminary results from their systematic reviews of the published and unpublished literatures that became early test examples for the development of the new system. A synthetic example disease along with simulated noisy and sparse data on that disease was also created to aid in testing. 
The broad principle behind this next generation of generic disease modeling can be characterized as connecting a system dynamics model to a statistical model, so that instead of doing forward simulation, as is traditionally the case in system dynamics modeling, the model describes an inverse problem. This approach is termed integrative systems modeling and its past applications are reviewed in the next section.
\section{Integrative systems modeling in other fields}
A vast body of literature exists on modeling system dynamics (Forrester, 1968) (Meadows, 2008) (Bossel, 2007).  In epidemiology, compartmental models are often constructed to simulate infectious disease dynamics (May & Anderson, 1991). The classic SIR model evolves a population through a “Susceptible” compartment to an “Infected” compartment to a “Recovered” compartment (Kermack & McKendrick, 1927). Infection dynamics are captured by making the amount of mass that moves from the “Susceptible” compartment to the “Infected” compartment dependent on the product of the masses in the two compartments. This dependence implies that the number of new infections will increase with the number of current infections. Extensions of this basic model abound (Daley & Gani, 2005) (Brauer & Castillo-Chavez, 2001). The transition parameters in this class of compartmental models, incidence and remission for instance in the case of the SIR model, are usually set based on extracting point estimates of the parameters from literature reviews. Uncertainty is usually assessed based on a sensitivity analysis that solves the compartmental model for the range of parameter estimates found in the literature (Nagelkerke, Jha, Vlas, Korenromp, Moses, & Blanchard, 2002) (Brandeau, Owens, Sox, & Wachter, 1993) (Broutin, Viboud, Grenfell, Miller, & Rohani, 2010). In the vast majority of statistical analyses, this estimation approach would not be considered sufficient. Instead, the analyst would attempt to find the parameters that, for instance, maximized the likelihood of a set of data samples of the parameter values. This statistical approach has the advantage that uncertainty can be rigorously quantified and an optimal estimate can be identified based on a transparent model. Nevertheless, the``plug-and-play’’ approach is widely accepted, primarily because of the difficulty of fitting this class of compartmental models.
Advances in statistical modeling and computation have allowed increasingly sophisticated models to be fit to data. These advances have spawned a new modeling approach that seeks to provide more reliable point estimates and estimates of uncertainty for parameters in compartmental models. This new approach, integrative systems modeling, connects a system dynamics model to a statistical model so that parameters in the system can be estimated in a statistical framework without sacrificing the structure provided by the dynamical model. 
The analyst building a statistical model has a rich vocabulary with which to describe the data generating process of interest. Data can come from a range of distributions. Hierarchical data can be expressed via random effects and smooth data through the correlation structure of a covariance matrix. In the most mature forms of integrated systems modeling, this rich vocabulary is made available for estimating parameters in a compartmental model. DisMod III is a prime example of connecting a sophisticated statistical model to the generic disease dynamics model. The complexity of the statistical model and the complexity of the underlying dynamic systems model vary across different applications. The field of pharmacokinetics and pharmacodynamics (PK/PD) provides examples of connecting a sophisticated statistical model to a compartmental model outside of the domain of generic disease modeling.  
Pharmacokinetics is the study of how drugs get absorbed and distributed in the body. Pharmacodynamics is the study of the effect of drugs on the body. Much of the content of these two fields overlap so they are often studied together. Within PK/PD, the field of population pharmacokinetics attempts to understand the sources of variability in drug response among individuals (Yuh, et al., 1994). Because clinical trials provide data on only a small subset of the target patient population and at small sample sizes, it is often difficult to estimate variation among individuals without imposing additional structure on the estimation problem. In 1972, the field of population pharmacokinetics began in earnest when nonlinear mixed effects modeling was proposed as a solution to the limited clinical data (Sheiner, Rosenberg, & Melmon, 1972). The techniques that have emerged in this field are mathematically identical to the new generation of generic disease modeling illustrated by DisMod III. Analysts in population pharmacokinetics connect a random effects model of patients within a study (the statistical model) to a compartmental model that describes the process of a drug’s absorption in the body (the dynamic systems model). 
Analogous to the advent of the DisMod software for simulating and estimating generic disease models, many different software packages have arisen to help conduct analyses in population pharmacokinetics. NONMEM, which developed at the UCSF, was one of the first (Beal, Sheiner, Boeckmann, & Bauer, 2009). SAAM II, a computer tool for the simulation, analysis and modeling of pharmacokinetic data, also allows users to fit compartmental models of the drug response to clinical data using the integrative systems modeling approach (Barrett, et al., 1998). One of the developers of that software, Brad Bell, has also contributed to the development of the DisMod family of software. The further exchange of insights in population pharmacokinetics and in generic disease modeling will likely benefit both fields in the future.


\section{History of Generic Disease Modeling}

DisMod, the original generic disease modeling system was developed
theoretically by Chris Murray (?) and implemented by TK in TK as part
of the original Global Burden of Disease Study in TK. TK Some
highlights of the original DisMod. Little was recorded about this
approach, although it is remembered fondly to this day by the
researchers involved.

For the second iteration of the Global Burden of Disease Study, Jan
van der TK, developed a new implementation of DisMod in TK Pascal,
which provided a stable user-interface and sufficient documentation
for the application to be used widely beyond the initial analysis.
DisMod II, like the original DisMod focused on data confrontation, and
specialized in combining single best-estimates of individual disease
parameters, such as incidence, prevalence, remission, and
case-fatality to produce a set of internally consistent estimates for
a single time, place, and sex.  This work was described in a series of
peer reviewed publications, TK, which also contrasted it with
alternative approaches.

As is often the case in science, a very similar approach had been
developed previously, by researchers totally unknown to the WHO
researchers, TK discussion of the Soviet dismod for terminal cancer
modeling from the 1970s.

This approach was distributed publicly by WHO and used widely in
burden of disease studies. TK some highlights, mentioning Theo Vos by
name. However, there were some feature requests and methodological
concerns that developed over the decade that DisMod II reigned as the
standard approach for disease burden estimation.

Chief among them was the difficultly in producing consistent estimates
that exhibited ``face validity'', for example age patterns that
increased monotonically as a function of age.  Despite strongly held
prior beliefs on the part of domain experts, it was not uncommon for
the data to show oscillations as a function of age, due to the
contortions that DisMod II would subject rates to in order to produce
consistent estimates as close to the inconsistent input estimates as
possible.

Another important challenge in the DisMod II workflow was the
production of single best estimates for at least 3 independent rates
in the first place.  As mentioned in section TK, for common diseases
like TK, there are over TK studies of disease prevalence that met the
inclusion criteria of a recent systematic literature review. DisMod II
provided no guidance on how to go from this large collection of
estimates, often for incommensurate age intervals, to a single best
estimate of disease prevalence.

Finally, although DisMod II excelled in providing consistent estimates
from the confrontation of inconsistent estimates of several disease
parameters for a single place and time, it was laborious on the part
of the data analyst to produce comparable estimates for a variety of
different places and times. In the Global Burden of Disease 2010
Study, there are 21 geographic regions to produce estimates for, at 3
different points in time, for males and females. Even an analysis that
is trivial for one region/time/sex becomes burdensome when it must be
replicated 126 times.

It was for these reasons that in 2008 I began a journey to develop a
new generation of generic disease modeling system. This work was
inspired by the previous generations of generic disease models
developed in the past global and national burden of disease studies,
as well as work on mortality estimation and prediction by Girotis
(SPELLING TK) and King, which used Bayesian methods to estimate age
patterns of mortality simultaneously for multiple regions of the
world. The model was developed in collaboration with many disease
experts who volunteered to help in the GBD 2010 study, an effort that
quickly because quite a massive undertaking.

The mental disorders group and substance dependence group were
particularly punctual in producing preliminary results from their
systematic reviews of the published and unpublished literature, and
therefore became the test examples in my development process (along
with a synthetic test example I developed, which I came to
affectionately call ``chronic dismoditis'').

Over the course of the 3 years I spent developing this approach, it
became clear that there is a broader principle which is behind the
model.  This can be characterized as connecting a system dynamics
model to a statistical model, so that instead of doing forward
simulation, as is traditionally the case in system dynamics modeling,
the model describes an inverse problem. This sort of inference on
compartmental models is rare in epidemiology, despite the long-running
focus of infectious disease modeling on compartmental formulations of
disease dynamics, and recently increased emphasis on ``model
calibration''. And to the best of my knowledge, it is rare in other
fields as well. My extensive web-searching discovered a recently
developed line of research into Bayesian calibration of computer
models, a handful of papers on ``plug-and-play'' modeling, and recurring
pleas that someone actually start doing inference on compartmental
models [refs TK].

\section{Integrative systems modeling in other fields}
There is only one field that I came across which has
systematically embraced the approach that I have taken here, which is
clinical pharmacokinetics. Since the 19XXs TK, PK/PD analysis has
developed appropriately complex compartmental models of human and
animal physiological systems and used measurements of pharmacological
data to fit the model parameters. The techniques emerging from this
field for so-called Population PK are mathematically identical to the
new generation of generic disease modeling and extensions that I have
stumbled upon. Connecting with the applied mathematicians like Brad
Bell, who worked on the initial development of the System for
Population Kinetics, have greatly assisted my understanding of the
technical issues at work in the generic disease model, and promise
exciting extensions in the future.

Wikipedia \url{http://en.wikipedia.org/wiki/Pharmacokinetics}:
\begin{quote}
Pharmacokinetics, sometimes abbreviated as PK, (from Ancient Greek
pharmakon ``drug'' and kinetikos ``to do with motion''; see chemical
kinetics) is a branch of pharmacology dedicated to the determination
of the fate of substances administered externally to a living
organism. The substances of interest include pharmaceutical agents,
hormones, nutrients, and toxins.

Pharmacokinetics is often studied in conjunction with
pharmacodynamics. Pharmacokinetics includes the study of the
mechanisms of absorption and distribution of an administered drug, the
rate at which a drug action begins and the duration of the effect, the
chemical changes of the substance in the body (e.g. by metabolic
enzymes such as CYP or UGT enzymes) and the effects and routes of
excretion of the metabolites of the drug.[1]

...

Population pharmacokinetics is the study of the sources and correlates
of variability in drug concentrations among individuals who are the
target patient population receiving clinically relevant doses of a
drug of interest.[12][13][14] Certain patient demographic,
pathophysiological, and therapeutical features, such as body weight,
excretory and metabolic functions, and the presence of other
therapies, can regularly alter dose-concentration relationships. For
example, steady-state concentrations of drugs eliminated mostly by the
kidney are usually greater in patients suffering from renal failure
than they are in patients with normal renal function receiving the
same drug dosage. Population pharmacokinetics seeks to identify the
measurable pathophysiologic factors that cause changes in the
dose-concentration relationship and the extent of these changes so
that, if such changes are associated with clinically significant
shifts in the therapeutic index, dosage can be appropriately modified.

Software packages used in population pharmacokinetics modeling include
NONMEM, which was developed at the UCSF.
\end{quote}

\section{The present project}
This book focuses on the statistical model and computational method
behind my new generic disease model, together with a series of
extensive case-studies of the model in action, culled from the massive
labors of the GBD2010. However, the computational infrastructure that
was required for the task should not be overlooked. Modern Bayesian
statistical methods are computationally intensive, and sharing
preliminary results with a globally distributed team of over 800
experts presents unique communications challenges, as well. Without
the existence of the easy-to-use web application development framework
Django, the statistical models and computational methods would have
been useless because no one would have been able to make use of them
in the timeframe available.  Similarly, without the expert assistance
of system adminstrators Trey TK and Serkan Yalchin, as well as
software engineer Jiaji Du and web designed (cum epidemiologist) Ben
Althouse, the models and methods would never have seen the light of
day.

Finally, the Bayesian models and methods used in this project relied
exclusively on the Python PyMC package [TK REF], developed by Chris
Fonnesbeck, David Huard, and Anand Patil. It would have been simply
impossible for me to put this together without their free/libre
open-source software package.
