\section{Statistical Models for Epidemiological Rates}
\label{theory-rate_model}

The key to connecting the systems dynamics model from
Chapter~\ref{theory-system_dynamics} to the evidence base collected
through systematic review is the \emph{rate model}.  This is a
statistical model, which has its core features defined by its
likelihood function.  Here a likelihood function is a probability
density function that assigns a value for the likelihood of every
possible rate value and uncertainty for a given parameter values.  An
examples will make this clearer, and for this I direct your attention
to the meta-analysis of population prevalence on Schizophrenia males.
The forest plot in Figure~\ref{fig:theory-rate_model-schiz_forest}
shows the results of combining $<<
len(d['schiz_forest.json|dexy']['r']) >>$ studies using $7$ different
rate models.  As the figure demonstrates, the choice of rate model can
have a huge effect on the estimated uncertainty, and can have a
noticable effect on the estimated median as well. In what follows, I
will develop a collection of rate models, starting with the simplest
and then increasing complexity, while identifying the benefits and
drawbacks of each.  The models to come, in order, are the binomial
model, the beta-binomial model, the poisson model, the negative
binomial model, and finally, several variants of normal models.

\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{theory-rate_model-schiz_forest_plot.png}
\end{center}
\caption{Forest plot summarizing $7$ alternative models for
  meta-analysis of population-level schizophrenia prevalence.  The
  population prevalence mean estimated by the models ranges from $<<
  d['schiz_forest.json|dexy']['min_est'] >>$ to $<<
  d['schiz_forest.json|dexy']['max_est'] >>$
  TK, and the width of the $95\%$ HPD interval of the population
  prevalence estimated by the models ranges from $<<
  d['schiz_forest.json|dexy']['min_spread'] >>$ to $<<
  d['schiz_forest.json|dexy']['max_spread'] >>$.}
\label{fig:theory-rate_model-schiz_forest}
\end{figure}

\subsection{Binomial Model}
My conceptually simplest model for rate data was built from the
Binomial random variable, which follows the probability distribution
\[
\Pr[X=k\given n,\pi] = \binom{n}{k}\pi^n(1-\pi)^{n-k}
\]
I've used greek to emphasize that $\pi$ is the parameter, while $n$
and $k$ are data.

Although this equation may appear opaque, the intuition behind it is
simple: $n$ individuals were tested for a disease, and $k$ tested
positive. The formula then follows from the assumption that each
individual tested positive with probability $\pi$, and that 
knowing about the test results of any subset of individuals gives me
no information about what the test results the others were
(these events are ``independent'').

This distribution inspires a computationally tractable and
theoretically appealing rate model for a observed population rate of
$r$ from a population of size $n$:
\[
\dens(p,n\given \pi) \propto \pi^{nr}(1-\pi)^{n(1-r)}.
\]
Note that it is not necessary to include the term $\binom{n}{nr}$,
because this does not depend on the model parameter $\pi$. There is a
constant of proportionality, which is necessary to make this rate
model truely a probability density function for any $\pi$, but happily
I will never need to know this constant, and I have use the ``proportional to''
symbol $\propto$ instead of equality to emphsize this fact.

The funnel plot in Figure~\ref{fig:theory-rate_model-binom_funnel}
shows the predictive distribution of this rate model for $\pi=<<
d['rate_model.json|dexy']['pi_binomial_funnel'] >>$.  It also shows
the potential problem with this approach: the data, as gathered from
systematic literature review are much more dispersed than this
distribution predicts.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\textwidth]{binomial-model-funnel.png}
\end{center}
\caption{Funnel plot showing predictive distribution for the binomial
  rate model with $\pi=<<
  d['rate_model.json|dexy']['pi_binomial_funnel'] >>$ (shown in blue),
  with empirical distribution of TK overlaid for comparison (shown in
  green).}
\label{fig:theory-rate_model-binom_funnel}
\end{figure}

All models are wrong, of course, so why does this model require
refinement? The answer is that is leads to unreasonably high confidence,
which I will now demonstrate.  If a study of $<< d['rate_model.json|dexy']['pop_A_N'] >>$
 people from in subpopulation A
finds $<< d['rate_model.json|dexy']['pop_A_prev']*100 >>\%$ prevalence and a
study of $<< d['rate_model.json|dexy']['pop_B_N'] >>$ people in subpopulation B
 finds $<< d['rate_model.json|dexy']['pop_B_prev']*100
>>\%$ prevalence, then the binomial model predicts that a study of
$<< d['rate_model.json|dexy']['pop_C_N'] >>$ people in subpopulation C will
 have $<< d['rate_model.json|dexy']['pop_C_prev']*100>>\%$ prevalence, with $95\%$ HPD interval
$<< d['rate_model.json|dexy']['pop_C_ui'] >>$.  I have no problem with the point
 estimate.  Picking the mean of the two populations seems just right.
 But the uncertainty interval lacks face validity.  It would be much
 more reasonable to have an uncertainty interval as large as $[5,45]$,
 instead of one as small as this.

One way to formalize this objection is through the posterior predictive
check, an in-sample goodness-of-fit test that can be done
graphically.  Figure~\ref{fig:theory-rate_model-binom_ppc} shows the
posterior predictions of the binomial model when it is fit to the TK
dataset, together with the data itself.  The model predictions are
clearly compressed, and trusting the results of such a model will lead
to inappropriate certainty in the face of noisy data.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\textwidth]{binomial-model-ppc.png}
\end{center}
\caption{Posterior predictive check for binomial model fit to TK
  data.  Blue shows 1000 draws from the posterior distribution of the
  binomial model, and green shows the input data.}
\label{fig:theory-rate_model-binom_ppc}
\end{figure}




\subsection{Beta-binomial model}
A theoretically appealing extension to the binomial model (which also
does not work for my purposes) is the beta-binomial model.  I will
develop it in this section, to motivate the following sections.

Formally, the beta binomial random variable is given by the following
probability distribution
\begin{align*}
\Pr[X = k\given n, \alpha, \beta] 
  &= \int_{\pi}\dens(\pi\given \alpha, \beta) \binom{n}{k}\pi^k(1-\pi)^{n-k}
d\pi\\
\dens(\pi\given \alpha, \beta) &\propto \pi^{\alpha-1}(1-\pi)^{\beta-1}
\end{align*}

The intuition behind this model is simpler than the equation, however.
Each individual now tests positive for the condition independently
with probability $\pi$ which is itself a random variable, a priori
distributed according to a beta distribution with parameters $\alpha$
and $\beta$. The beta distribution is given by 
\[
\dens(\pi\given \alpha, \beta) =
\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\pi^{\alpha-1}(1-\pi)^{\beta-1}
\]
and has a high degree of flexibility, as well as the appropriate
support, always taking values between zero and one.
Figure~\ref{fig:theory-rate_model-beta} shows the probability density
of the beta distribution for several combinations of $\alpha$ and
$\beta$.
\begin{figure}[ht]
\begin{center}
\includegraphics[width=\textwidth]{beta-distribution.png}
\end{center}
\caption{Probability density for the beta distribution for a range of
  $\alpha$ and $\beta$ values. Dashed black line shows expected value,
which is $.5$ for all distribution in a) and $.25$ for all in b).}
\label{fig:theory-rate_model-beta}
\end{figure}

The beta binomial distribution inspired the following rate model for
an observed population rate of $r$ from a population of size $n$:
\[
\dens(r,n\given \alpha, \beta) \propto \int_{\pi}
\pi^{\alpha-1}(1-\pi)^{\beta-1} \pi^k (1-\pi)^{n-k}
d\pi.
\]

This model extends the binomial model in a way analogous to how a
random effects model in linear regression.  By introducing an
additional dimension in to the parameter space, it is able to capture
the dispersion beyond the binomial model that I have observed
empirically in funnel plots of real data
(Figure~\ref{fig:theory-rate_beta-binomial-funnel} shows the beta
binomial funnel plot, as well as the posterior predictive check for
this model on the same data as used in
Figure~\ref{fig:theory-rate_model-binom_ppc}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\textwidth]{beta-binomial-funnel.png}
\end{center}
\caption{Funnel plot and posterior predictive check for beta binomial model.}
\label{fig:theory-rate_beta-binomial-funnel}
\end{figure}

This model addresses the theoretical shortcoming raised in the
previous section: if two observations show TK $1\%$ and TK $3\%$ prevalence
respectively, the posterior distribution of the beta binomial model
has mean $2\%$ and 95\% HPD interval TK $[1\%,3\%]$, which seems quite
reasonable.

The great shortcoming of the beta binomial model is computational.  As
will be elaborated in Chapter~\ref{theory-numerical_algorithms}, there
is no closed-form solution to the integral in the probability density
for the beta binomial model.  Evaluating it requires introducing a
latent variable for each of the data points in the likelihood.  This
simply has too much cost for the numerical algorithms and
computational infrastructure available to me currently.  So my search
continues.

\subsection{Approximation to the binomial model}
There are two traditional approximations to the binomial distribution,
depending on how large $k$ is in relation to $n$.  When $k/n$ is
large, the normal distribution is used, and when $k/n$ is small, the
binomial is similar to the Poisson distribution.

Since I expect to usually be in a ``small $k/n$'' setting, I will not
develop the normal model in detail now, although I will return to a
generalization of the normal distribution which subsumes it later.

The Poisson distribution is given by the the equation
\[
\Pr[X=k] = \frac{\lambda^k e^{-\lambda}}{k!},
\]
and it can be understood intuitively as the number of times a
``memoryless'' event occurs in a unit time period.  Setting $\lambda =
\pi n$ produces an approximation to the binomial distribution, which
is quite accurate for large $n$ and small $k$.
Figure~\ref{fig:theory-rate_model-poisson_approx_to_binom}
demonstrates how precise this approximation can be.

\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{poisson_approx_to_binom.png}
\end{center}
\caption{The Poisson distribution approximates the binomial distribution very closely. TK more description in caption.}
\label{fig:theory-rate_model-poisson_approx_to_binom}
\end{figure}

Because of the similarity of these distributions, a Poisson model,
defined formally below, is subject to all of the concerns raised about the
binomial model regarding inappropriately low levels of uncertainty
when modeling rates with non-sampling variation at the level typically
gathered in systematic review.  For completeness, however, here is the Possion model:
\[
\dens(r,n\given \pi) \propto (\pi n)^{\lfloor rn\rfloor} e^{-\pi n}.
\]
There is one key benefit to this model, compared to the binomial and
beta-binomial models, however.  The Poisson model assigns a
theoretically justified and non-zero likelihood to rates of more than
one.  Although prevalence is always less than one, it is theoretically
possible to have incidence rates more than one, and remission rates
are more than one (per person-year) quite frequently in practice.

The other benefit from this approach is found in the the
over-dispersed variant of the Poisson distribution.  This distribution
is called the negative binomial distribution (named after the formula
that the proves it indeed sums to one) and, unlike the beta-binomial
distribution, it \emph{does} have a closed form (if you consider the gamma function ``closed''):
\[
\Pr[X = k\given \pi, \rho] = \frac{\Gamma(k+\rho)}{\Gamma(\rho)k!} (1-\pi)^\rho \pi^k.
\]

However, this closed form obscures the intuition behind this model,
which is more relevant to the present discussion.
There is a simple intuitive description of the negative-binomial
distribution, in terms of a hierarchical model.  It is similar to the
intuition behind the beta-binomial distribution. Through a bit of
algebra, the negative binomial distribution can be represented as a
hierarchical model, where the observed data comes from a Poisson
distribution, and the parameter of the Poisson distribution is itself
a random variable that comes from a gamma distribution:
\begin{align*}
X\given \pi &\sim \Poisson(\pi)\\
\pi &\sim \GammaDist(\mu, \delta)
\end{align*}
Here the Gamma distribution is defined by $\dens(x; k,\theta) =
x^{k-1} \frac{e^{-x/\theta}}{\theta^k \, \Gamma(k)}$ TK which can be
conveniently parameterized in terms of its mean and precision as TK.
Through this lens, the negative binomial model can be interpreted as a
nature adaptation of the traditional random effects model in linear
regression to the Poisson case, where each observation comes from a
different Poisson model and the Poisson parameter of these models are
all drawn from a common Gamma distribution.

Thus a rate models based on it provides benefits in handling
non-sampling variation similar to those demonstrated for the beta
binomial distribution above, but in a formulation that is much less
demanding computationally.  The negative-binomial rate model for
observing a rate of $r$ in a population of size $n$ is:
\[
\dens(r,n\given \pi, \rho) \propto \frac{\Gamma(rn+\rho)}{\Gamma(\rho)} (1-\pi)^\rho \pi^{rn}.
\]

A difficultly I have observed in practice with the negative binomial
model is in the realm of numerical algorithms.  My experience has been that these
likelihoods lead to models that are simply harder to fit than
traditional normal models.  Nonetheless, the negative binomial model
has been the primary tool in the likelihod modeling for the
applications to be presented in Chapters~\ref{practice-all-examples}.

Chapter~\ref{TK} will examine the properties of numerical algorithms
for fitting the negative binomial model in more detail, but one
approach which has helped substantially is imposing an informative
prior on the over-dispersion parameter of the negative binomial
distribution.  Because it seems unreasonable to impose a very informative prior on the over-dispersion parameter, I have limited the possibilities to augmenting the hierarchical formulation of the negative binomial model above with the following:
\[
\log_{10} \delta \sim \Normal\left(\mu_{\log_10\delta}, .25^2\right),
\]
with $\mu_{\log_10\delta} = 1, 2, 3$.  The results of adding this
prior is contrasted with the results vanilla negative binomial model
(with an uninformative prior on $\delta$) in
Figure~\ref{fig:theory-rate_model-neg_binom_priors}, which shows that
a belief a priori that the observations is more over-dispersed leads
to a posterior estimate where the predicted rate have more
uncertainty, as expected.  TK short discussion of its salubrious
effects on the convergence of MCMC.

\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{neg_binom_priors.png}
\end{center}
\caption{TK description of the simulation experiment to understand effect of prior on negative binomial model.}
\label{fig:theory-rate_model-neg_binom_priors}
\end{figure}

\subsection{Alternative Models}
Some epidemiological data is not count data at all.  Duration studies
are and measurements of the relative risk of mortality are two that
come up frequently in systematic review.  For duation data, a normal
model  has been quite sufficient, and a log-normal model has proven
appropriate for modeling relative risk data, which can be thought of
as a ratio of count variables.

Normal, log-normal, logit-normal, probit-normal, etc models have also
been used for mortality rates in the past [ref TK], and are worthy of
continued consideration for modeling the incidence, prevalence, etc.,
as an alternative to the negative binomial.

TK formal definition of a transformed normal model.

TK drawbacks of common transforms.

A particular variant of the log-normal model, which I am not aware of
from the literature, is what might be called the offset log
transformed model.  The problem with the log normal model (and the
logit normal, probit normal, etc) is dealing with zero counts in the
data.  Operationally, this is because $\log(0) = -\infty$, and
practically it is because the probability of observing a zero
count is zero, but it happens frequently.  There are two common
methods to fix this, dropping all zeros, and adding a small offset.
Dropping measurements of zero is clearly problematic, as it leads to
systematic bias in the data that remains and produces estimates larger
than they should be.  This is especially true for high quality studies
which focus on the age pattern of a disease, where it is quite
reasonable for some age groups to have zero cases observed.  The
effect of dropping zeros is to overestimate the rates in these age
groups.

Adding a small amount, such as $0.5$ is an alternative solution, and
indeed, this is the approach taken for cause-specific mortality estimation
in GK [ref TK].  The selection of the offset can appear ad hoc,
however.  There is a theoretical justification which can aid in
selection, however, and it is based on a decomposition of the
measurement error into an additive part and a multiplicative part.

TK deriveation and explanation of the offset log-transformed model

TK funnel plot and comparison of OLT, NB, BB.

TK discusson of priors on the offset parameter.

\subsection{Summary and Comparison}
TK summary with a table showing all of the approaches above, positives
and negatives

TK some sort of comparison: simulation study? posterior predictive
checks? something analogous to SMART counties in small areas
estimation?

Age range example:
\url{
http://www.rachel.org/lib/meta_leukemias_near_nukes.070701.pdf
}
