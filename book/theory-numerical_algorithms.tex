\chapter{Numerical Algorithms}
\label{numerical-algorithms}

Computational tractability has an important influence on model
development, which often goes unacknowledged. The models that I fit
are often a compromise between models I would like to fit and the
limitations of the algorithms and computing infrastructure
available. This has always been the case, but modern algorithms and
modern computing have shifted the balance tremendously.

In the days before digital computers, computational tractability meant
that models must be simple and computational methods elegant. In the
18th century, for example, an important challenge in predictive
modeling was in navigation \cite{Williams_From_1993}. Forecasting the
path of stars allowed a ship to chart its course accordingly. The
method of least squares, first published at the start of that century
by Legendre, elegantly provided a solution
\cite{Legendre_Nouvelles_2011}\. The method finds an approximate
solution to a set of equations by minimizing the square sum of the
residuals. Using this method, one can plot the location of a celestial
body at different time points, postulate a model form (for instance by
assuming that the body follows a straight line), and then use the
method of least squares to determine the parameters of the model that
best fit the data. Why minimize the square sum of the residuals? Why
not minimize the distance between the data and a proposed solution or
the sum of the distance and the number of parameters in the model? For
one, it has attractive theoretical properties, because it is
equivalent to finding parameters of maximum likelihood given normally
distributed data. But perhaps just as importantly, minimizing the
square sum permits a closed form, analytical solution that was well
matched to the computational resources of the day.

With the development of digital computation, more computationally
intensive methods have become feasible. When Ulam challenged himself
to calculate the probability of winning in solitaire in the 1940s, an
analytic solution was elusive. A simple, but computationally intensive
approximation method was developed. As Ulam later remarked, ``The
question was what are the chances that a Canfield solitaire laid out
with 52 cards will come out successfully? After spending a lot of time
trying to estimate them by pure combinatorial calculations, I wondered
whether a more practical method than `abstract thinking' might not
be to lay it out say one hundred times and simply observe and count
the number of successful plays''. This approach has been
generalized to the Monte Carlo method, a class of computational
methods that rely on repeated random sampling to approximate
calculations that are intractable to calculate exactly
\cite{Eckhardt_Stan_1987}.

It is the successors to the Monte Carlo algorithm that make the
Bayesian methods I use in integrative systems modeling possible.  In
Bayesian terms, the model of process and model of data articulated in
the first half of this book provide a prior distribution and
likelihood.  In equations, it is a simple application of Bayes'
formula to go from this to the posterior distribution.  The exact
computation of the distribution is intractable, however, and it is
algorithms for sampling from the posterior distribution (or a close
approximation thereof) that produce the parameter estimates for my
models.

Bayesian methods were developed contemporaneously to the method of
least squares, but were limited in application before the development
of Markov chain Monte Carlo (MCMC) algorithms and modern computers.
Prior to these innovations, analysis was only tractable for a very
limited class of prior distributions and likelihoods. With sufficient
computing power, the posterior distribution can be sampled from using
Monte Carlo methods instead of being computed analytically
\cite{Gelman_Bayesian_2003}. Monte Carlo methods can also be applied
to integrate the posterior distribution to obtain, for instance, the
posterior mean and variance. As computational resources to apply the
approach to more complex problems have become more widely available,
the approach has gained popularity \cite{Tanner_From_2010}.

My integrative systems model of disease in a population does not admit
a closed form representation for its posterior distribution.  Instead,
I rely on MCMC to draw samples of the model parameters from their
posterior distribution.  This, too, requires some care.  The
statistical computation tradition has put a lot of effort into
deriving Gibbs samplers for specific Bayesian models, while
theoretical computer scientists have focused on developing generic
algorithms like the ``Ball Walk'' for sampling from convex sets.  I
use the Metropolis step method, and the Adaptive Metropolis (AM)
variant \cite{Haario_Adaptive_2001}, which, in practice, provides
acceptible performance without requiring burdensome derivation of
customized Gibbs distributions. The MCMC algorithm benefits from
wisely chosen initial values, and this seems to be particularly true
when using MCMC with the AM step method in a large parameter space. I
use Powell's method, which optimizes a function of many variables
without requiring derivatives, to find initial value for the model
parameters for MCMC \ref{MJD Powell, An efficient method for finding
  the minimum of a function of several variables without calculating
  derivatives , The Computer Journal (1964) 7 (2): 155-162.  doi:
  10.1093/comjnl/7.2.155}.  I use Normal Approximation at this initial
value to find initial values for the variance-covariance matrices in
the AM step method.  Furthermore, I use an \emph{empirical Bayes}
approach to separate the global model into submodels that can be fit
in parallel.  The remainder of this chapter describes each aspect of
the numerical algorithm in more detail.

\section{Markov chain Monte Carlo}
Markov chain Monte Carlo (MCMC) is a class of Monte Carlo methods that
obtains approximate solutions using a carefully designed Markov
chain. A Markov chain is a stochastic process, or a sequence of random
variables, such that the probability distribution of a random variable
at one point in the sequence only depends on the random variable
immediately before it in the sequence. If a Markov chain satisfies
certain conditions (ergodicity) then it must tend towards a unique stationary
distribution as the sequence continues. The key to using the MCMC
algorithm for integrative systems modeling is constructing a Markov
chain with the following three properties:
\begin{enumerate}
\item the stationary distribution of the chain is equal to the
  posterior distribution of the model
\item each step of the chain can be computed efficiently
\item the chain converges to its stationary distribution in a
  reasonable number of steps
\end{enumerate}

A simple example can make this clearer. Suppose I want to sample
uniformly from the unit ball in $n$ dimensions, meaning the set of
points $\{x \in \bbR^n: \|x\| \leq 1\}$.  The MCMC approach starts
from any point in the ball, for example the origin $X_0 = (0, \ldots,
0)$, and generates successive points $X_1, X_2, \ldots$ randomly, so
that the points are Markovian, which is to say the probability density
of $X_{i+1}$ is dependent only on the value of $X_i$.  There is great
art to designing the probability density which produces $X_{i+1}$.  In
this case, the Gibbs step is a simple one: choose an axis $e_i$
uniformly from the basis $\{e_1, e_2, \ldots, e_n\}$, and then choose
$X_{i+1}$ uniformly from the interval given by the
intersection of the ball with the line parallel to $e_i$ that passes through $X_i$.

To be precise, this Markov chain has transition probability density given by
\[
\dens(X_{i+1}=x|X_i) = 
\begin{cases}
\frac{1}{n}\cdot\frac{1}{\sqrt{1-\sum_{j \neq d} x_j^2}}, &\quad\|x\| \leq 1 \text{ and } x_j = X_{i,j} \text{ for all }j \neq d;\\
0, &\quad\text{otherwise.}
\end{cases}
\]

To see that the stationary distribution of the chain is equal to the
uniform distribution requires a small calculation, TK.

Implementing each step of the chain is simple in this case, requiring
only a way to choose numbers uniformly from the interval $[0,1]$
(which is \emph{not} simple, but it a basic primitave that randomized
computation relies on; I use the Mersenne Twister pseudorandom number
generator, a well-tested standard \cite{TK M. Matsumoto and T. Nishimura,
  “Mersenne Twister: A 623-dimensionally equidistributed uniform
  pseudorandom number generator”, ACM Transactions on Modeling and
  Computer Simulation Vol. 8, No. 1, January pp.3-30 1998.}.)

An elegant proof that this chain converges quickly to its stationary
distribution follows the coupling method, TK.

\section{Adaptive Metropolis-Hastings MCMC}
As mentioned above, my approach to parameter estimation with MCMC does
\emph{not} rely on deriving Gibbs step methods, which are often much
more involved that the simple example in the previous section.  I rely
heavily on the Metropolis-Hastings step method, and an adaptive
variant thereof.

In the context of Bayesian statistics, the Metropolis-Hastings
algorithm is a technique used to sample from the posterior
distribution when the posterior distribution cannot be easily sampled
from directly. The algorithm selects the next step of its random walk
in two steps: first it makes a proposal by choosing from a proposal
probability distribution, which depends on the current value of the
walk. Then it accepts or rejects this proposal with a probability
carefully designed to yield the desired stationary distribution.

In the example from the previous section, uniform sampling from the
unit ball, the proposal distribution could be a normal distribution centered at the current value, for example
\[
P_i \sim \Normal\left(X_i, \sigma^2\right).
\]
The Metropolis rejection rule is based on the quantity $p_i =
\frac{\dens(P_i)\dens'(P_i|X_i)}{\dens(X_i)\dens'(X_i|P_i)}$, where
$\dens(\cdot)$ is the posterior probability density for value $x$, and
$\dens'(p, x)$ is the probability density of proposing $p$ when the
chain has value $x$.  The rejection rule is the following:
\[
X_{i+1} = \begin{cases}
P_i, &\quad\text{with probability } p_i;\\
X_i, &\quad\text{with probability } 1-p_i.
\end{cases}
\]
This simplifies considerable when sampling from the unit ball with the
symmetric proposal distribution above, to simply
\[
X_{i+1} = \begin{cases}
P_i , &\quad\text{if }\|P_i\| \leq 1;\\
X_i, &\quad\text{otherwise}.
\end{cases}
\]

TK example application to sampling from a banana in the plane.

Adaptive Metropolis-Hastings extends Metropolis-Hastings by adaptively
adjusting the variance-covariance matrix for the proposal
distribution, based on the acceptance rate of the proposals.  TK more
correct and mathematical description of this. If few proposals are
getting accepted, then the algorithm increases the variance of the
proposal distribution in order to consider more directions in the
random walk. If a lot of proposals are getting accepted, then the
algorithm decreases the variance of the proposal distribution in order
to zero in on the right direction. DisMod 3 was developed using PyMC,
a Python library that implements the Adaptive Metropolis-Hastings
algorithm as well as many other MCMC methods \cite{Patil_PyMC_2010}.

TK example of sampling from a banana with AM step method.

TK a word about convergence, the theoretical applicability of the AM
method, practical tests of convergence, the need for experimental
testing and validation.  At least 3 general approaches exist for
making a Markov chain's convergence more likely. The first and
simplest approach is to just run the chain for longer and thus take
more samples. The second approach is to use a more appropriate step
method, for example by using Adaptive Metropolis-Hasting over the
Metropolis-Hasting algorithm. Other more complicated step methods can
also be used, but they tend to be less stable. Adaptive
Metropolis-Hasting is one of the more stable, multi-purpose step
methods available.

\section{Initial Values from MAP}
Shorter run time for better initial values. For instance, by
initializing certain parameters to their maximum likelihood values
\cite{Bishop_Neural_1995}.

TK description of powell's method

TK example of fitting a simple model with and without good initial
values.

\section{Empirical Bayes}

This technique falls under the rubric of Empirical Bayes.

Empirical Bayesian approaches estimate the prior distributions in the
model instead of specifying entirely uninformative prior
distributions. In the context of descriptive epidemiology,
appropriately informative prior distributions help guide the model
towards realistic estimates of epidemiological parameters. Without
Empirical prior distributions, the data are often too noisy and sparse
to yield sensible estimates alone or the computational cost of
obtaining sensible estimates would be too large, given the time it
would take for the Markov chain to converge. Only the data combined
with the system dynamics model and an appropriate Empirical prior can
give reliable estimates with current computational constraints.

TK formal definition of what I am doing when I do empirical bayes.

Many challenges remain in fitting complex integrated systems
models. In the early period of regression analysis, the optimization
routines used to find the maximum likelihood estimate of a model's
parameters required constant tweaking and observation in order to
converge. Now, these techniques are incredibly robust and applicable
to a wide range of modeling problems. We are still in a relatively
early stage of fitting complex Bayesian models using Monte Carlo
methods. As computing power increases and these computational methods
progress, modelers will have access to increasingly more reliable and
general purpose fitting algorithms and with that access will come
increasingly realistic and predictive models.

TK example of empirical bayes, comparison to fully Bayesian approach.

\subsection{Computational aspects of forward simulation}
There are several computational optimizations of the forward
simulation that are discussed in this section.  Because the
statistical inference technique will require simulating the
compartmental model for thousands or millions of different parameter
settings, speed is of the essence.  TK discussion of different
approaches to solving the differential equations: exact solution,
numerical solution. Stiff versus non-stiff numerical
solution. Approximation when remission rate is very high.

\section{Testing and validation}

\subsection{Implementation Testing}
TK simulation study to show that model performs as expected when model is
correctly specified.

\subsection{Sensitivity of model assumptions}
TK a series of models for simulated data that demonstrates how the
model responds to violation of model assumptions.

\subsection{What happens to prevalence when the disease incidence or remission or mortality is not constant over time?}

Certain important diseases such as diabetes are widely believed to
have substantial changes in incidence, remission, and mortality rates
over time.  What is the effect of the endemic equilibrium assumption
of the age-specific prevalence? TK plots comparing prevalence from a
synthetic cohort model to a period model using simulation data.


\section{Future Directions for Research}

MCMC has been an enabler for this approach.  Without free/libre
open-source software for implementing AM/MCMC this project would not
have been possible.  But it is not the only approach.  Message passing
algorithms have proven themselves TK more on this.  Nonlinear
optimization, is another promising approach TK especially combined
with bootstrap method for estimating uncertainty.  Population monte
carlo
